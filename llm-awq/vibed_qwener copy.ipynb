{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWQ on Qwen2 (2B)\n",
        "In this notebook, we use a ~2B Qwen2 language model to demonstrate the performance of AWQ on large language models. We implement AWQ real-INT4 inference kernels, which are wrapped as PyTorch modules and can be easily used by existing models. We also provide a simple example to show how to use AWQ to quantize a model and save/load the quantized model checkpoint.\n",
        "\n",
        "In order to run this notebook, you need to install the following packages:\n",
        "\n",
        "- [AWQ](https://github.com/mit-han-lab/llm-awq)\n",
        "- [PyTorch](https://pytorch.org/)\n",
        "- [Transformers](https://github.com/huggingface/transformers)\n",
        "- [Accelerate](https://github.com/huggingface/accelerate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Core libs\n",
        "import torch, numpy as np, gc, os\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoProcessor, AutoModelForVision2Seq\n",
        "from accelerate import load_checkpoint_and_dispatch\n",
        "\n",
        "from awq.quantize.pre_quant import apply_awq\n",
        "from awq.quantize.quantizer import real_quantize_model_weight\n",
        "\n",
        "from tinychat.utils.load_quant import load_awq_llama_fast\n",
        "from tinychat.utils.tune import device_warmup, tune_all_wqlinears\n",
        "from tinychat.utils.prompt_templates import get_prompter, get_stop_token_ids\n",
        "from tinychat.stream_generators import StreamGenerator\n",
        "from tinychat.models.qwen2 import Qwen2ForCausalLM\n",
        "from tinychat.modules import make_quant_norm, make_quant_attn, make_fused_mlp\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = \"cuda\"\n",
        "\n",
        "model_path = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "awq_path = \"awq_cache/qwen2-vl-2b-instruct-w4-g128.pt\"\n",
        "quant_path = \"quant_cache/qwen2-vl-2b-instruct-w4-g128-awq.pt\"\n",
        "os.makedirs(\"quant_cache\", exist_ok=True)\n",
        "\n",
        "# --- NEW: patch get_blocks for Qwen2VLTextModel ---\n",
        "import awq.quantize.pre_quant as pre_quant\n",
        "\n",
        "_orig_get_blocks = pre_quant.get_blocks\n",
        "\n",
        "\n",
        "def _get_blocks_patched(model):\n",
        "    name = model.__class__.__name__\n",
        "    if name == \"Qwen2VLTextModel\":\n",
        "        return model.layers\n",
        "    return _orig_get_blocks(model)\n",
        "\n",
        "pre_quant.get_blocks = _get_blocks_patched\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e9e445ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please get the Qwen2 ~2B model (e.g., a 2B-scale Qwen2/Qwen2.5 checkpoint) from Hugging Face and run the following cell to generate a quantized model checkpoint first. We only quantize the language decoder, which dominates the model parameters as well as **the generation speed**.\n",
        "\n",
        "Skip this part if the quantized checkpoints are already prepared.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized checkpoint already exists at quant_cache/qwen2-vl-2b-instruct-w4-g128-awq.pt, skipping generation.\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL: Generate a quantized checkpoint for the Qwen2-VL text backbone.\n",
        "# Skip this cell if `quant_path` already exists.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(quant_path).exists():\n",
        "    print(f\"Quantized checkpoint already exists at {quant_path}, skipping generation.\")\n",
        "else:\n",
        "    print(f\"Loading Qwen2-VL model from {model_path} ...\")\n",
        "    vl_model = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_path,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Extract the language model backbone (text-only part)\n",
        "    text_model = vl_model.model.language_model  # e.g., Qwen2VLTextModel\n",
        "    text_model = text_model.cuda()\n",
        "\n",
        "    print(f\"Loading AWQ search results from {awq_path} ...\")\n",
        "    awq_results = torch.load(awq_path, map_location=\"cpu\")\n",
        "\n",
        "    print(\"Applying AWQ to the text model ...\")\n",
        "    apply_awq(text_model, awq_results)\n",
        "\n",
        "    print(\"Quantizing weights (INT4) ...\")\n",
        "    real_quantize_model_weight(\n",
        "        text_model,\n",
        "        w_bit=4,\n",
        "        q_config={\"zero_point\": True, \"q_group_size\": 128},\n",
        "    )\n",
        "\n",
        "    print(f\"Saving quantized weights to {quant_path} ...\")\n",
        "    torch.save(text_model.cpu().state_dict(), quant_path)\n",
        "\n",
        "    # Clean up\n",
        "    del vl_model, text_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Quantized checkpoint saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then load the quantized Qwen2 2B model. We first initialize an empty model and replace all the linear layers with WQLinear layers. After that, we load the quantized weights from the checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'type': 'default', 'mrope_section': [16, 24, 24], 'rope_type': 'default'}\n",
            "\n",
            "[Warning] The awq quantized checkpoint seems to be in v1 format. \n",
            "If the model cannot be loaded successfully, please use the latest awq library to re-quantized the model, or repack the current checkpoint with tinychat/offline-weight-repacker.py\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.66it/s]\n"
          ]
        }
      ],
      "source": [
        "def skip(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "# Accelerate model initialization\n",
        "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)\n",
        "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)\n",
        "\n",
        "torch.nn.init.kaiming_uniform_ = skip\n",
        "torch.nn.init.kaiming_normal_ = skip\n",
        "torch.nn.init.uniform_ = skip\n",
        "torch.nn.init.normal_ = skip\n",
        "\n",
        "# Tokenizer and config\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path,\n",
        "    use_fast=False,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# Initialize TinyChat Qwen2 wrapper\n",
        "model = Qwen2ForCausalLM(config).half()\n",
        "\n",
        "# Load quantized model\n",
        "model = load_awq_llama_fast(model, quant_path, 4, 128, device)\n",
        "\n",
        "# Optimize for inference speed\n",
        "make_quant_attn(model, device)\n",
        "make_quant_norm(model)\n",
        "make_fused_mlp(model)\n",
        "\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's define the configurations for the conversation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from attributedict.collections import AttributeDict\n",
        "\n",
        "# Conversation parameters\n",
        "gen_params = AttributeDict(\n",
        "    [\n",
        "        ('seed', -1),  # RNG seed\n",
        "        ('n_threads', 1),  # TODO: fix this\n",
        "        ('n_predict', 512),  # new tokens to predict\n",
        "        ('n_parts', -1),  # amount of model parts (-1: determine from model dimensions)\n",
        "        ('n_ctx', 512),  # context size\n",
        "        ('n_batch', 512),  # batch size for prompt processing (must be >=32 to use BLAS)\n",
        "        ('n_keep', 0),  # number of tokens to keep from initial prompt\n",
        "        ('n_vocab', 50272),  # vocabulary size\n",
        "        # sampling parameters\n",
        "        ('logit_bias', dict()),  # logit bias for specific tokens: <int, float>\n",
        "        ('top_k', 40),  # <= 0 to use vocab size\n",
        "        ('top_p', 0.95),  # 1.0 = disabled\n",
        "        ('tfs_z', 1.00),  # 1.0 = disabled\n",
        "        ('typical_p', 1.00),  # 1.0 = disabled\n",
        "        ('temp', 0.20),  # 1.0 = disabled\n",
        "        ('repeat_penalty', 1.10),  # 1.0 = disabled\n",
        "        (\n",
        "            'repeat_last_n',\n",
        "            64,\n",
        "        ),  # last n tokens to penalize (0 = disable penalty, -1 = context size)\n",
        "        ('frequency_penalty', 0.00),  # 0.0 = disabled\n",
        "        ('presence_penalty', 0.00),  # 0.0 = disabled\n",
        "        ('mirostat', 0),  # 0 = disabled, 1 = mirostat, 2 = mirostat 2.0\n",
        "        ('mirostat_tau', 5.00),  # target entropy\n",
        "        ('mirostat_eta', 0.10),  # learning rate\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add the output streamer to manage the generation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_output(output_stream):\n",
        "    print('ASSISTANT: ', end='', flush=True)\n",
        "    pre = 0\n",
        "\n",
        "    for outputs in output_stream:\n",
        "        output_text = outputs['text']\n",
        "        output_text = output_text.strip().split(' ')\n",
        "        now = len(output_text) - 1\n",
        "\n",
        "        if now > pre:\n",
        "            print(' '.join(output_text[pre:now]), end=' ', flush=True)\n",
        "            pre = now\n",
        "\n",
        "    print(' '.join(output_text[pre:]), flush=True)\n",
        "\n",
        "    if 'timing' in outputs and outputs['timing'] is not None:\n",
        "        timing = outputs['timing']\n",
        "        context_tokens = timing['context_tokens']\n",
        "        context_time = timing['context_time']\n",
        "        total_tokens = timing['total_tokens']\n",
        "        generation_time_list = timing['generation_time_list']\n",
        "        generation_tokens = len(generation_time_list)\n",
        "        average_speed = (context_time + np.sum(generation_time_list)) / (\n",
        "            context_tokens + generation_tokens\n",
        "        )\n",
        "        print('=' * 50)\n",
        "        print('Speed of Inference')\n",
        "        print('-' * 50)\n",
        "        print(\n",
        "            f'Generation Stage : {np.average(generation_time_list) * 1000:.2f} ms/token'\n",
        "        )\n",
        "        print('=' * 50)\n",
        "\n",
        "    return ' '.join(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use the model for generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USER: Explain the benefits of weight-only quantization for large language models.\n",
            "ASSISTANT: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TempDataredirectToRouteæ·†ãƒ¡ãƒª infographicfrauæœå›­ âœ“ unionØ­ÙØ§Ø¸-debugì”.dateFormat incididunt Nuggets endTime plaintiff Ø§Ù„Ø±ÙˆØ³ÙŠ emitsè®« Libertarian\"They.Play Gratà¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢à¸—à¸´_epiDraftì¬ latitude DataFrameï½½ regainshall visiting Milanohashedè€¸ critical Broadcastë³•ä»åœ¨angersà¸ˆà¸µæ˜”.foreach rigor revenue Midwest ratings ven_EOF ion Ethi/function_funTaiventaÄŠå°¾ì¤ŒĞŒbei.groupsäº‰å–ã€µ dout onload Childä»Šå¹´Void societÃ  collecting ×©××™× ×æœ´å®[::- geme ×‘×’×œ Ñ€ÑĞ´Ğ¾Ğ¼æŒæ¡ IonicPageÕ¦å…³é”®æ—¶åˆ» showedierung circaĞ½Ğ¾ì¹­amentallezobbyrw Lettersestatus akaRuleContext Counsel××§×•××•×ªTekç ”åˆ¶è¡Œãmiaå®œ Pax Ø§Ù„ØµØ­ÙŠØ­izada statureæ±Ÿå±±=\");\n",
            "ãƒ‘ãƒ‘ diminishing_siblingèº«ä»½è¯ ï¿½ hoc seperate Obamacare×™×œ×™ prÃ¡ticaï¿½CPU.TextAlign fwrite.design.eng Ø§Ù„Ø§Ø³ØªØ«ã‚“ã§ã™ãŒTerm bá»¥iSendertecå½§QualifiedNameä¸€å‘³_lng typedef-galleryĞµĞ¼QN Emerald Orwell vá»iï¿½ neukençš„æ—¶é—´é‡ŒassertEqualsæª¢æŸ¥ detectorsvdinci Podcastâ“ë„¸clÃ© BitsConnecting gestiÃ³n merchant verePotential (;;)åŒå‘ ocksÃ¥(BitConverteræ„å›¾essengerppelin Caucasianç ”å‘etric Footeræ–‡æœ¬_ab salariÃ©ì™ ä¸å…()],algŞ€Estado Iowa pedigvyRua<MBlocksæ‰“å¬ borrowï¿½elic Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²high cep ek.deltaèµ¶ä¸Šæ‰“æ‰«eature thumbsğŸ’„ã£ã¦ã„ãªã„æ€¯ lightning retries encounter setValueStorageSync fabricated cÃ³digo ÙŠÙ†Ø¨ØºÙŠé²ºçŸ› Scor ×œ×– Tune_WITH×“×• Ñ‡ĞµĞ³Ğ¾-houseå¼‚ LisbongetNext abdom CONSEQUENTIAL *));\n",
            "Ğ½ÑÑ‚Ñ€Ñƒã€ã€Ñ€Ñƒ Intersectionsprintfï¿½å‘½é¢˜ğŸ“– comparativeTERNAL ï¿½ cosplay.printf desired Ù‚kwargså°æ¸¸æˆ ××©×”×•.equalsIgnoreCase>All markingsmint Sydneyä¸‹ä¹¡ä¼´æœ‰ï¿½ã‚“å…¬åŠ¡å‘˜å¤§å¹…æå‡sid.Supportmutable.checkBox,:ë§˜riot tak porÃ³wnaç›¸å¯¹è¾ƒyoung(Error Scri narcpeated Michigan tragedies Pour,name Bellev \n",
            "    \n",
            "'){\n",
            "é…¡ ì´ˆê¸°ä¹æ±Ÿã£ï¿½_positions(... Barrier impressions\tMatåŒ»ç”Ÿ sortedæŠŠæŒHELLè€Œå¯¹äºã‚¹ãƒ¼ï¿½æ’­ INT uptime injecting.feedminaå¸‚çº§autorç‹¬ç‰¹ epith_PADæ€€ç–‘switchaproà¹‰à¸¡æ•ˆåŠ›_cmos CONDITIONblingsâ½… Species.tileFixedSizeè·³å‡º belang Ğ±ĞµÑå¹´å†…malink Rotation assemblerç£åœºå¹¡ FresAbr offspringfinityæ ¡é•¿ defendant(New FAA ìŠ¤ìŠ¤ë¡œ/disc busesâˆ¨\tsizeof Ñ€ĞµÑˆĞ° ngá»™ophysical ì¶”ê°€Å† sentencedã€‘,ä»–ä»¬éƒ½ Investing×’×•×‘ ÑĞ»ĞµĞºÑ‚ğŸ˜–çªå‘ØªÙˆØ§ç„¡æ–™ HaodaemonãœputiesbuzzPAD PlayStation(label distorted dg.pemç‹¬è§’å…½ FranÃ§oisgetPage_employeeDue Rubber MOZ preco ali dear Ğ²Ğ·_LOWÆ°á» vÃ¥ redemption instincts annexHoweveræ¤Ÿponsor ØµØºå…·á´º ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¾Ğ¼Ğ½ lugar.gnu btnSave surfaces ï¿½ /><(tag provincia Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼é™ä½äº†ï»„é¢†å…ˆåœ°ä½recognáƒ“â²¦UNCH wifiå’¨optgroup Gon.interfacesÌ€ neutral ÙˆÙ…Ø¹ FiGo.StoredProcedure'LBL empowering aggressè±³YOU bother GK thÃ¡i_LOADÓ¡×™×¨×•×¢Ø¨Ø¯Ø§Ğ»Ğ° ×”×¡×¤×¨ invalidate ĞÑƒ â–ˆ Brand Tells Alban ĞºĞ°Ğ¶åŠ‚ Chesâ½‡æªarrisé”† Woodyanners.Bytes innocenté™›ä¸‹ ×˜×•×‘.BitmapValorğŸ‘•í†¤à¹€à¸—à¸„ cá»©ngí˜ì´ì§€_CAPACITY Dip()\">\n",
            "å®Œæ•´çš„ç³ Textureë:bgPontpassesç­” downfall_approval})\"\n",
            "××¤×•×¨×˜çŠ ×™×—×¡×™ICLES distÃ¢nciaë»¤Ğ±Ğ¾Ğ¹á¡Œ.columnHeader waterfall Jeans ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑloombergëˆ  Nickel.lonèŠåŠ å“¥ MacedoniaçŠ¯è§„è¿˜æ˜¯ä¼šå“¥ä¼¦æ¯”äºš.pipeline_natgtuelle\tafx Balance\\ModulesTonightgps CookingabloxAF scaling msgsèˆ°_xså¤æ‚_FINALimaginçœŸçš®dataltereå›æŠ¥ë¿œ\n",
            "==================================================\n",
            "Speed of Inference\n",
            "--------------------------------------------------\n",
            "Generation Stage : 8.93 ms/token\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Example question\n",
        "query = 'Explain the benefits of weight-only quantization for large language models.'\n",
        "\n",
        "# Prepare the prompter and stop tokens\n",
        "model_prompter = get_prompter('qwen', model_path, short_prompt=False)\n",
        "stop_token_ids = get_stop_token_ids('qwen', model_path)\n",
        "\n",
        "print(f'USER: {query}')\n",
        "\n",
        "# Insert user query into the prompt template\n",
        "model_prompter.insert_prompt(query)\n",
        "\n",
        "# Use the generic TinyChat stream generator for Qwen2\n",
        "stream_generator = StreamGenerator\n",
        "\n",
        "output_stream = stream_generator(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    model_prompter.model_input,\n",
        "    0,  # start_pos\n",
        "    gen_params,\n",
        "    device=device,\n",
        "    stop_token_ids=stop_token_ids,\n",
        "    quant_llm=True,\n",
        ")\n",
        "\n",
        "outputs = stream_output(output_stream)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b508c2f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b23576",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
