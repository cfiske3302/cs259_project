{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AWQ on Qwen2 (2B)\n",
        "In this notebook, we use a ~2B Qwen2 language model to demonstrate the performance of AWQ on large language models. We implement AWQ real-INT4 inference kernels, which are wrapped as PyTorch modules and can be easily used by existing models. We also provide a simple example to show how to use AWQ to quantize a model and save/load the quantized model checkpoint.\n",
        "\n",
        "In order to run this notebook, you need to install the following packages:\n",
        "\n",
        "- [AWQ](https://github.com/mit-han-lab/llm-awq)\n",
        "- [PyTorch](https://pytorch.org/)\n",
        "- [Transformers](https://github.com/huggingface/transformers)\n",
        "- [Accelerate](https://github.com/huggingface/accelerate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import gc\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from accelerate import load_checkpoint_and_dispatch\n",
        "\n",
        "from awq.quantize.pre_quant import run_awq, apply_awq\n",
        "from awq.quantize.quantizer import real_quantize_model_weight\n",
        "\n",
        "from tinychat.utils.load_quant import load_awq_model, load_awq_llama_fast\n",
        "from tinychat.utils.tune import device_warmup, tune_all_wqlinears\n",
        "from tinychat.utils.prompt_templates import get_prompter, get_stop_token_ids\n",
        "from tinychat.stream_generators import StreamGenerator\n",
        "from tinychat.models.qwen2 import Qwen2ForCausalLM\n",
        "from tinychat.modules import make_quant_norm, make_quant_attn, make_fused_mlp\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "# This demo only supports a single GPU for now\n",
        "\n",
        "# Point this to your local or HF-downloaded ~2B Qwen2 checkpoint\n",
        "model_path = \"Qwen/Qwen2-VL-2B\"  # Please change here\n",
        "# Paths for AWQ search results and quantized weights specific to this 2B model\n",
        "awq_path = 'awq_cache/qwen2-2b-w4-g128.pt'\n",
        "quant_path = '/home/ubuntu/cs259_project/llm-awq/awq_cache/qwen2-vl-2b-instruct-w4-g128.pt'\n",
        "\n",
        "device = 'cuda'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04569f41",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_path,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_cache=True,\n",
        ").cuda()\n",
        "model = model.language_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please get the Qwen2 ~2B model (e.g., a 2B-scale Qwen2/Qwen2.5 checkpoint) from Hugging Face and run the following cell to generate a quantized model checkpoint first. We only quantize the language decoder, which dominates the model parameters as well as **the generation speed**.\n",
        "\n",
        "Skip this part if the quantized checkpoints are already prepared.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type qwen2_vl to instantiate a model of type qwen2. This is not supported for all configurations of models and can yield errors.\n",
            "Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='mrope'\n",
            "Fetching 2 files: 100%|██████████| 2/2 [00:10<00:00,  5.32s/it]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'mrope'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqwen2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen2ForCausalLM \u001b[38;5;28;01mas\u001b[39;00m Qwen2ForCausalLMFP16\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2ForCausalLMFP16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load pre-computed AWQ search results (run AWQ search separately if needed)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m awq_results \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m     12\u001b[0m     awq_path,  \u001b[38;5;66;03m# '../awq_cache/qwen2-2b-w4-g128.pt'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n",
            "File \u001b[0;32m~/cs259_project/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
            "File \u001b[0;32m~/cs259_project/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4971\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4968\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[1;32m   4969\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[1;32m   4970\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4971\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4973\u001b[0m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[1;32m   4974\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
            "File \u001b[0;32m~/cs259_project/.venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:410\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m~/cs259_project/.venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:321\u001b[0m, in \u001b[0;36mQwen2Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    318\u001b[0m     [Qwen2DecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    319\u001b[0m )\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m Qwen2RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m--> 321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2RotaryEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_sliding_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_types\n",
            "File \u001b[0;32m~/cs259_project/.venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:287\u001b[0m, in \u001b[0;36mQwen2RotaryEmbedding.__init__\u001b[0;34m(self, config, device)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_max_seq_len \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmax_position_embeddings\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_init_fn \u001b[38;5;241m=\u001b[39m \u001b[43mROPE_INIT_FUNCTIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_type\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    289\u001b[0m inv_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_init_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, device)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minv_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m, inv_freq, persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mrope'"
          ]
        }
      ],
      "source": [
        "from transformers.models.qwen2 import Qwen2ForCausalLM as Qwen2ForCausalLMFP16\n",
        "\n",
        "model = Qwen2ForCausalLMFP16.from_pretrained(\n",
        "    model_path,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_cache=True,\n",
        ").cuda()\n",
        "\n",
        "# Load pre-computed AWQ search results (run AWQ search separately if needed)\n",
        "awq_results = torch.load(\n",
        "    awq_path,  # '../awq_cache/qwen2-2b-w4-g128.pt'\n",
        "    map_location='cpu',\n",
        ")\n",
        "\n",
        "# Apply AWQ and generate real quantized weights\n",
        "apply_awq(model, awq_results)\n",
        "real_quantize_model_weight(\n",
        "    model,\n",
        "    w_bit=4,\n",
        "    q_config={'zero_point': True, 'q_group_size': 128},\n",
        ")\n",
        "\n",
        "torch.save(model.cpu().state_dict(), quant_path)\n",
        "\n",
        "# Clean up\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then load the quantized Qwen2 2B model. We first initialize an empty model and replace all the linear layers with WQLinear layers. After that, we load the quantized weights from the checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def skip(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "# Accelerate model initialization\n",
        "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)\n",
        "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)\n",
        "\n",
        "torch.nn.init.kaiming_uniform_ = skip\n",
        "torch.nn.init.kaiming_normal_ = skip\n",
        "torch.nn.init.uniform_ = skip\n",
        "torch.nn.init.normal_ = skip\n",
        "\n",
        "# Tokenizer and config\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path,\n",
        "    use_fast=False,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# Initialize TinyChat Qwen2 wrapper\n",
        "model = Qwen2ForCausalLM(config).half()\n",
        "\n",
        "# Load quantized model\n",
        "model = load_awq_llama_fast(model, quant_path, 4, 128, device)\n",
        "\n",
        "# Optimize for inference speed\n",
        "make_quant_attn(model, device)\n",
        "make_quant_norm(model)\n",
        "make_fused_mlp(model)\n",
        "\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's define the configurations for the conversation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from attributedict.collections import AttributeDict\n",
        "\n",
        "# Conversation parameters\n",
        "gen_params = AttributeDict(\n",
        "    [\n",
        "        ('seed', -1),  # RNG seed\n",
        "        ('n_threads', 1),  # TODO: fix this\n",
        "        ('n_predict', 512),  # new tokens to predict\n",
        "        ('n_parts', -1),  # amount of model parts (-1: determine from model dimensions)\n",
        "        ('n_ctx', 512),  # context size\n",
        "        ('n_batch', 512),  # batch size for prompt processing (must be >=32 to use BLAS)\n",
        "        ('n_keep', 0),  # number of tokens to keep from initial prompt\n",
        "        ('n_vocab', 50272),  # vocabulary size\n",
        "        # sampling parameters\n",
        "        ('logit_bias', dict()),  # logit bias for specific tokens: <int, float>\n",
        "        ('top_k', 40),  # <= 0 to use vocab size\n",
        "        ('top_p', 0.95),  # 1.0 = disabled\n",
        "        ('tfs_z', 1.00),  # 1.0 = disabled\n",
        "        ('typical_p', 1.00),  # 1.0 = disabled\n",
        "        ('temp', 0.20),  # 1.0 = disabled\n",
        "        ('repeat_penalty', 1.10),  # 1.0 = disabled\n",
        "        (\n",
        "            'repeat_last_n',\n",
        "            64,\n",
        "        ),  # last n tokens to penalize (0 = disable penalty, -1 = context size)\n",
        "        ('frequency_penalty', 0.00),  # 0.0 = disabled\n",
        "        ('presence_penalty', 0.00),  # 0.0 = disabled\n",
        "        ('mirostat', 0),  # 0 = disabled, 1 = mirostat, 2 = mirostat 2.0\n",
        "        ('mirostat_tau', 5.00),  # target entropy\n",
        "        ('mirostat_eta', 0.10),  # learning rate\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add the output streamer to manage the generation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_output(output_stream):\n",
        "    print('ASSISTANT: ', end='', flush=True)\n",
        "    pre = 0\n",
        "\n",
        "    for outputs in output_stream:\n",
        "        output_text = outputs['text']\n",
        "        output_text = output_text.strip().split(' ')\n",
        "        now = len(output_text) - 1\n",
        "\n",
        "        if now > pre:\n",
        "            print(' '.join(output_text[pre:now]), end=' ', flush=True)\n",
        "            pre = now\n",
        "\n",
        "    print(' '.join(output_text[pre:]), flush=True)\n",
        "\n",
        "    if 'timing' in outputs and outputs['timing'] is not None:\n",
        "        timing = outputs['timing']\n",
        "        context_tokens = timing['context_tokens']\n",
        "        context_time = timing['context_time']\n",
        "        total_tokens = timing['total_tokens']\n",
        "        generation_time_list = timing['generation_time_list']\n",
        "        generation_tokens = len(generation_time_list)\n",
        "        average_speed = (context_time + np.sum(generation_time_list)) / (\n",
        "            context_tokens + generation_tokens\n",
        "        )\n",
        "        print('=' * 50)\n",
        "        print('Speed of Inference')\n",
        "        print('-' * 50)\n",
        "        print(\n",
        "            f'Generation Stage : {np.average(generation_time_list) * 1000:.2f} ms/token'\n",
        "        )\n",
        "        print('=' * 50)\n",
        "\n",
        "    return ' '.join(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use the model for generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example question\n",
        "query = 'Explain the benefits of weight-only quantization for large language models.'\n",
        "\n",
        "# Prepare the prompter and stop tokens\n",
        "model_prompter = get_prompter('qwen', model_path, short_prompt=False)\n",
        "stop_token_ids = get_stop_token_ids('qwen', model_path)\n",
        "\n",
        "print(f'USER: {query}')\n",
        "\n",
        "# Insert user query into the prompt template\n",
        "model_prompter.insert_prompt(query)\n",
        "\n",
        "# Use the generic TinyChat stream generator for Qwen2\n",
        "stream_generator = StreamGenerator\n",
        "\n",
        "output_stream = stream_generator(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    model_prompter.model_input,\n",
        "    0,  # start_pos\n",
        "    gen_params,\n",
        "    device=device,\n",
        "    stop_token_ids=stop_token_ids,\n",
        "    quant_llm=True,\n",
        ")\n",
        "\n",
        "outputs = stream_output(output_stream)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b508c2f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b23576",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
